#!/usr/bin/env python3
"""
GPUåˆ©ç”¨ç‡ä¼˜åŒ–å™¨
"""
import torch
import os
from loguru import logger

def optimize_gpu_performance():
    """ä¼˜åŒ–GPUæ€§èƒ½è®¾ç½®"""
    try:
        logger.info("å¼€å§‹GPUæ€§èƒ½ä¼˜åŒ–...")
        
        # 1. è®¾ç½®CUDAä¼˜åŒ–ç¯å¢ƒå˜é‡
        os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # å¼‚æ­¥æ‰§è¡Œï¼Œæé«˜ååé‡
        os.environ['TORCH_USE_CUDA_DSA'] = '0'    # ç¦ç”¨è®¾å¤‡ç«¯æ–­è¨€ï¼Œæé«˜æ€§èƒ½
        os.environ['CUDA_CACHE_DISABLE'] = '0'    # å¯ç”¨CUDAç¼“å­˜
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'  # ä¼˜åŒ–å†…å­˜åˆ†é…
        
        # 2. è®¾ç½®PyTorchä¼˜åŒ–
        torch.backends.cudnn.benchmark = True     # å¯ç”¨cuDNNè‡ªåŠ¨è°ƒä¼˜
        torch.backends.cudnn.deterministic = False # å…è®¸éç¡®å®šæ€§ç®—æ³•ï¼Œæé«˜æ€§èƒ½
        torch.backends.cuda.matmul.allow_tf32 = True  # å¯ç”¨TF32ï¼Œæé«˜æ€§èƒ½
        torch.backends.cudnn.allow_tf32 = True    # å¯ç”¨cuDNN TF32
        
        # 3. è®¾ç½®å†…å­˜ç®¡ç†
        if torch.cuda.is_available():
            # è®¾ç½®æ›´é«˜çš„å†…å­˜ä½¿ç”¨ç‡
            torch.cuda.set_per_process_memory_fraction(0.95)  # ä½¿ç”¨95%æ˜¾å­˜
            torch.cuda.empty_cache()
            
            # è·å–GPUä¿¡æ¯
            gpu_name = torch.cuda.get_device_name(0)
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"GPU: {gpu_name}")
            logger.info(f"æ€»æ˜¾å­˜: {total_memory:.2f}GB")
            logger.info(f"æ˜¾å­˜ä½¿ç”¨ç‡: 95%")
        
        logger.info("GPUæ€§èƒ½ä¼˜åŒ–å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"GPUæ€§èƒ½ä¼˜åŒ–å¤±è´¥: {e}")
        return False

def test_gpu_utilization():
    """æµ‹è¯•GPUåˆ©ç”¨ç‡"""
    try:
        if not torch.cuda.is_available():
            logger.error("CUDAä¸å¯ç”¨")
            return False
        
        logger.info("æµ‹è¯•GPUåˆ©ç”¨ç‡...")
        
        # åˆ›å»ºå¤§é‡è®¡ç®—ä»»åŠ¡æ¥æµ‹è¯•GPUåˆ©ç”¨ç‡
        device = torch.device('cuda')
        
        # åˆ›å»ºå¤§çŸ©é˜µè¿›è¡Œè®¡ç®—
        size = 4096
        a = torch.randn(size, size, device=device, dtype=torch.float16)
        b = torch.randn(size, size, device=device, dtype=torch.float16)
        
        # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
        for i in range(10):
            c = torch.matmul(a, b)
            torch.cuda.synchronize()  # åŒæ­¥ç­‰å¾…å®Œæˆ
        
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
        memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
        
        logger.info(f"GPUå†…å­˜ä½¿ç”¨: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB")
        
        # æ¸…ç†å†…å­˜
        del a, b, c
        torch.cuda.empty_cache()
        
        logger.info("GPUåˆ©ç”¨ç‡æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"GPUåˆ©ç”¨ç‡æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    logger.info("=" * 60)
    logger.info("GPUåˆ©ç”¨ç‡ä¼˜åŒ–å™¨")
    logger.info("=" * 60)
    
    # ä¼˜åŒ–GPUæ€§èƒ½
    if optimize_gpu_performance():
        logger.info("âœ… GPUæ€§èƒ½ä¼˜åŒ–æˆåŠŸ")
    else:
        logger.error("âŒ GPUæ€§èƒ½ä¼˜åŒ–å¤±è´¥")
        exit(1)
    
    # æµ‹è¯•GPUåˆ©ç”¨ç‡
    if test_gpu_utilization():
        logger.info("âœ… GPUåˆ©ç”¨ç‡æµ‹è¯•æˆåŠŸ")
    else:
        logger.error("âŒ GPUåˆ©ç”¨ç‡æµ‹è¯•å¤±è´¥")
        exit(1)
    
    logger.info("=" * 60)
    logger.info("ğŸš€ GPUä¼˜åŒ–å®Œæˆï¼ç°åœ¨åº”è¯¥èƒ½è·å¾—æ›´é«˜çš„åˆ©ç”¨ç‡")
    logger.info("=" * 60)




